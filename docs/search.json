[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Like many technologists, I build light weight experiments to learn new technology and build out a value hypothesis for them.\nI share a few of them on this blog that I think may be of general interest.\nViews expressed on the site are mine.\nThe blog is built using Quarto."
  },
  {
    "objectID": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html",
    "href": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html",
    "title": "POC: Using an LLM to convert a set of policy documents into a training presentation for a specific role",
    "section": "",
    "text": "Many companies in regulated spaces need to maintain written policies or operating procedures, e.g. Security Policies, HIPAA Policies, Backup Policies, etc. These companies also need to ensure that their employees are trained on these policies.\nIdeally this would be done with role specific training material that is generated by content experts.\nHowever for many smaller companies, this training involves the employees reading the policy document directly and recording that event. This is done mainly because creating and maintaining training material are generally cost prohibitive for these companies.\nThis is non optimal for various reasons:\n\nPolicy documents are written for a wide audience, covering all possible cases, readers have to read through large amounts of text to sift out the details relevant to their role\nDocuments are sometimes written in legal or regulatory language that is hard to read and comprehend\nEvery time a policy changes the whole process has to be repeated, there is no possibility of incremental training\n\nThe ability of LLMs to parse, summarize, and classify documents could be used to create a process that generates training material on demand that solves for the above short comings."
  },
  {
    "objectID": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#why",
    "href": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#why",
    "title": "POC: Using an LLM to convert a set of policy documents into a training presentation for a specific role",
    "section": "",
    "text": "Many companies in regulated spaces need to maintain written policies or operating procedures, e.g. Security Policies, HIPAA Policies, Backup Policies, etc. These companies also need to ensure that their employees are trained on these policies.\nIdeally this would be done with role specific training material that is generated by content experts.\nHowever for many smaller companies, this training involves the employees reading the policy document directly and recording that event. This is done mainly because creating and maintaining training material are generally cost prohibitive for these companies.\nThis is non optimal for various reasons:\n\nPolicy documents are written for a wide audience, covering all possible cases, readers have to read through large amounts of text to sift out the details relevant to their role\nDocuments are sometimes written in legal or regulatory language that is hard to read and comprehend\nEvery time a policy changes the whole process has to be repeated, there is no possibility of incremental training\n\nThe ability of LLMs to parse, summarize, and classify documents could be used to create a process that generates training material on demand that solves for the above short comings."
  },
  {
    "objectID": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#how",
    "href": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#how",
    "title": "POC: Using an LLM to convert a set of policy documents into a training presentation for a specific role",
    "section": "How ?",
    "text": "How ?\nFor this POC we will confirm the ability of a LLM to extract appropriate content from policy documents and format it in a way that could be used to generate a presentation.\nLocal llm model will be used because of regulatory and legal concerns for most companies in this space.\nUse the simplest approach that can prove out the concept\n\nSetup up a local model\nFor the POC lets set up an appropriate size model on a development laptop. The laptop I am using is a Apple M2 Max with 64 GB memory.\nThere are multiple methods to setup local models, for convenience we will be using Ollama\n\n%pip install ollama\n\n\nimport ollama\n\n\n!ollama pull llama3.1\n\n\nstream = ollama.chat(\n    model='llama3.1', \n    messages=[ {\n    'role': 'user',\n    'content': 'Why is the sky blue?',\n  },\n],\nstream=True)\nfor chunk in stream:\n    print(chunk['message']['content'], end='', flush=True)\n\nThe sky appears blue to us because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the atmosphere. Here's a simplified explanation:\n\n1. **Sunlight enters Earth's atmosphere**: The sun emits white light, which contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n2. **Light is scattered by gas molecules and particles**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2), as well as smaller particles like dust, water vapor, and pollutants.\n3. **Short-wavelength light is scattered more**: The shorter-wavelength colors (like blue and violet) are scattered more than the longer-wavelength colors (like red and orange). This is because the smaller molecules and particles in the atmosphere are better at scattering these shorter wavelengths.\n4. **Blue light reaches our eyes**: As a result of this scattering, the sky appears blue to us because we see more of the blue light that's being scattered towards us than any other color.\n\nSome additional factors contribute to the sky's appearance:\n\n* **Time of day**: During sunrise and sunset, the sky can take on hues of red and orange due to the scattering of longer wavelengths by atmospheric particles.\n* **Atmospheric conditions**: Pollution, dust, and water vapor in the atmosphere can scatter light differently, affecting the apparent color of the sky.\n* **Earth's atmosphere thickness**: The Earth's atmosphere is relatively thin at higher altitudes, so the blue color is more pronounced when looking towards the horizon.\n\nThere you have it – a simplified explanation for why the sky appears blue!\n\n\nWe have a local model installed and verified to be working\n\n\nRead the docs and Index them\nInput policy docs are in markdown format and in a common directory. We will use the llamaindex library to read and index these files.\nWe could just use SimpleDirectoryReader and read the entire directory and index it. But we will be creating an index per document.\nThe reasoning here is that many of these documents have related concepts and we’d like to keep the generated training material as close to the contents of the individual documents as possible.\nThe same effect could be had with a RAG across all documents with metadata but this seems simpler for such short documents (Max size 25 pages)\n\nimport os\nimport glob\n\n\ndata_files = glob.glob(os.path.join(\"data/itpolicy\", \"*.md\"))\n\n\n\nfrom llama_index.llms.ollama import Ollama\nfrom llama_index.readers.file import FlatReader\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core import Settings\n\nUse llamaindex global settings to configure a local llm.\nNotes: * I am not using json output because it generated variable keys based on input context. Markdown output was easier to process into the presentation. * Ollama will use the M2 gpu automatically\n\nSettings.llm = Ollama(model='llama3.1', json_mode=False, temperature=0)\nSettings.embed_model = \"local\"\n\nFunction that returns a query engine over the the datafile you’re querying. Query engine is a llamaindex context explained here\n\ndef get_query_engine(data_file):\n    docs = FlatReader().load_data(data_file)\n    index = VectorStoreIndex.from_documents(docs, parser=MarkdownNodeParser())\n    return index.as_query_engine(similarity_top_k=3)\n\n\n\nCreate a prompt\nThe obvious first prompt was\nCreate training material for a software engineer based on this document\nThat didn’t work because the model persisted in returning instructions for creating training material rather than the training material itself. This is the step that took a bunch to time to get right. The prompt that mostly reflected what I was looking for is below.\n\nprompt = \"\"\"\n    Using data from the context document generate a document of the following format:\n    # Importance to Company\n        Important reasons why this this policy, is important to the company, in a bulleted list.\n    # Important Concepts\n        Important concepts covered in this policy, with a brief description, in a bulleted list.\n    # Compliance for Software Engineers\n        Bulleted list of key points on how a software engineer can stay compliant with this policy.\n    # Additional Information\n        Where an engineer could get more information about this policy.\n    Do not quote external information directly.\n    \"\"\"\n \n\n\nfrom pathlib import Path\n\n\n\nRun the prompt\n\nRead each document.\nCreate a query engine per document.\nRun the prompt on the query engine.\n\n\nall_resp = {}\nfor data_file in data_files:\n    query_engine = get_query_engine(Path(data_file))\n    response = query_engine.query(f'{prompt}')\n    all_resp[data_file] = response\n\n\n\nValidate output\n\nimport re\nfile_name = list(all_resp.keys())[0]\nprint(re.split(r\"# Importance to Company|# Important Concepts\", all_resp[file_name].response)[1])\n\n\n* Maintaining trust and confidence among customers and stakeholders is crucial for the company's reputation and business continuity.\n* Ensuring the security and privacy of sensitive data protects the company from financial losses, legal liabilities, and damage to its brand.\n\n\n\n\nVisual confirmation that the approach could work to generate training content. We do need to validate how good this content is, we will leave it for a later exercise.\nI looked at more than a couple of lines :)\nWe will go into more elaborate validation approaches in a subsequent post.\n\n\nGenerate the presentation\nUsing the python-pptx library to generate a basic presentation.\nThis is clearly not production code.\nIn real use, converting the model to a semantic representation, to decouple the extraction of training content from its use, would be the way to go.\n\n%pip install python-pptx\n\n\nfrom pptx import Presentation\nfrom pptx.util import Pt\nfrom pptx.enum.text import PP_ALIGN\n\ndef add_text(title, font_size, p):\n    run = p.add_run()\n    run.text = title\n    font = run.font\n    font.size = Pt(font_size)\n\ndef create_slide_with_title(prs, title, layout = 1, font_size = 22):\n    slide = prs.slides.add_slide(prs.slide_layouts[layout])\n    shape = slide.shapes.title\n    text_frame = shape.text_frame\n    p = text_frame.paragraphs[0]\n    if layout == 0:\n        p.alignment = PP_ALIGN.CENTER\n    else:\n        p.alignment = PP_ALIGN.LEFT\n    add_text(title, font_size, p)\n    return slide\n\ndef add_list_to_slide(slide, para, level = 0, font_size = 20):\n    shape = slide.placeholders[1]\n    text_frame = shape.text_frame\n    p = text_frame.add_paragraph()\n    p.alignment = PP_ALIGN.LEFT\n    p.level = level\n    add_text(para, font_size, p)\n    p.space_after = Pt(font_size)\n\ndata_files.sort() \nprs = Presentation()\nfor data_file in data_files:\n    bname = os.path.basename(data_file)\n    resp = all_resp[data_file]\n    create_slide_with_title(prs, bname, layout=0, font_size = 24) # will need to clean up file name         \n    lines = resp.response.split(\"\\n\")\n    for i, line in enumerate(lines):\n        if line.rstrip().lstrip() == \"\":\n            continue\n        if line.startswith(\"#\"):\n            slide = create_slide_with_title(prs, line.replace(\"#\", \"\"), layout = 1, font_size = 22)\n            continue\n        if line.startswith(\"* \"):\n            add_list_to_slide(slide, line.replace(\"*\", \"\"), level = 1, font_size = 18)\n            continue\n        add_list_to_slide(slide, line.replace(\"*\", \"\"), level = 0, font_size = 20)\n\nprs.save(\"output.pptx\")"
  },
  {
    "objectID": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#what-next",
    "href": "posts/training-generator-1/10_ollama_with_llama3 _training_material_generation.html#what-next",
    "title": "POC: Using an LLM to convert a set of policy documents into a training presentation for a specific role",
    "section": "What Next ?",
    "text": "What Next ?\n\nAutomate validation of generated content.\nAdd ability to generate quizzes based on training content\nExpand the process to work with incremental changes to policy documents\nAdd integration into opensource learning management systems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "conceptualight",
    "section": "",
    "text": "POC: Using an LLM to convert a set of policy documents into a training presentation for a specific role\n\n\n\n\n\n\njupyter\n\n\npython\n\n\nllama3.1\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\nNo matching items"
  }
]